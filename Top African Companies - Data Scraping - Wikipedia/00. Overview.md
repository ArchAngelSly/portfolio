# Executive Summary
This project focuses on extracting and analysing data of the top 100 African companies listed on Wikipedia. A Python script was developed in Jupyter Notebook to:<br>
1.	Scrape the required data using the BeautifulSoup and requests libraries.<br>
2.	Generate a comprehensive data profiling report using the pandas-profiling module.<br>

The project is a hands-on demonstration of web scraping, data transformation, and profiling techniques. It highlights the ability to automate data collection and conduct exploratory data analysis efficiently.
________________________________________
# Methodology
## Step 1: Define the Objective
•	**What Was Done:** Clearly define the goal to extract and analyse the top 100 African companies listed on Wikipedia.<br>
•	**Inputs:** URL of the Wikipedia page containing the table of top 100 African companies.<br>
•	**Tools Used:** Jupyter Notebook for scripting.<br>
•	**Outputs:** A roadmap for data scraping and analysis.<br>
________________________________________
## Step 2: Web Scraping
•	**What Was Done:** Extract the table of top 100 African companies from the Wikipedia page.<br>
•	**Inputs:** Wikipedia URL and specific HTML table structure.<br>
•	**Tools Used:** <br>
o	*Python modules:* BeautifulSoup and requests.<br>
o	Browser's "Inspect Element" feature to locate the correct table.<br>
•	**Code Generated:** see “Top African Companies – Data Scraping – Wikipedia.ipynb”<br>
•	**Outputs:** Extracted data stored as “Top 100 African Companies.csv”.<br>
________________________________________
## Step 3: Data Profiling
•	**What Was Done:** Generate a Pandas Profiling report to understand the dataset better.<br>
•	**Inputs:** “Top 100 African Companies.csv”.<br>
•	**Tools Used:** <br>
o	*Python modules:* pandas and pandas-profiling.<br>
o	Jupyter Notebook for profiling script.<br>
•	**Code Generated:** see “Top African Companies – Data Scraping – Wikipedia.ipynb”<br>
•	**Outputs:**<br>
o	Pandas Profiling Report saved as “Top African Companies – Pandas Profilin Report.html”.<br>
________________________________________
## Step 4: Documentation
•	**What Was Done:** Documented the process and outputs for reproducibility.<br>
•	**Tools Used:** Markdown in Jupyter Notebook for inline comments and explanations.<br>
________________________________________
# Challenges Encountered
There were no significant challenges encountered. However, correctly identifying the target table on the Wikipedia page required trial-and-error, which was resolved by inspecting the HTML structure using browser developer tools.
________________________________________
# Summary of Skills and Experience Practised
## 1.	Web Scraping: 
o	Learned to extract HTML elements using BeautifulSoup.<br>
o	Familiarised with the structure and navigation of web pages.<br>
## 2.	Data Transformation: 
o	Parsed HTML tables into structured Pandas DataFrames.<br>
o	Exported data to CSV for further analysis.<br>
## 3.	Data Profiling: 
o	Generated detailed insights into the dataset using pandas-profiling.<br>
o	Practiced handling exploratory data analysis tools.<br>
## 4.	Problem-Solving: 
o	Resolved challenges in identifying the correct HTML table.<br>
o	Automated repetitive tasks like data extraction and reporting.<br>
## 5.	Technical Documentation: 
o	Demonstrated the ability to document processes and share reproducible workflows.


